@incollection{LeCun:1998:CNI:303568.303704,
	author = {LeCun, Yann and Bengio, Yoshua},
	chapter = {Convolutional Networks for Images, Speech, and Time Series},
	title = {The Handbook of Brain Theory and Neural Networks},
	editor = {Arbib, Michael A.},
	year = {1998},
	isbn = {0-262-51102-9},
	pages = {255--258},
	numpages = {4},
	url = {http://dl.acm.org/citation.cfm?id=303568.303704},
	acmid = {303704},
	publisher = {MIT Press},
	address = {Cambridge, MA, USA},
} 



@incollection{NIPS2012_4824,
	title = {ImageNet Classification with Deep Convolutional Neural Networks},
	author = {Alex Krizhevsky and Sutskever, Ilya and Hinton, Geoffrey E},
	booktitle = {Advances in Neural Information Processing Systems 25},
	editor = {F. Pereira and C. J. C. Burges and L. Bottou and K. Q. Weinberger},
	pages = {1097--1105},
	year = {2012},
	publisher = {Curran Associates, Inc.},
	url = {https://goo.gl/UpFBv8}
}


@Book{mile,
	author = 	 {Mile Šikić, Mirjana Domazet-Lošo},
	title = 	 {Bioinformatika },
	publisher = 	 {Bioinformatics - course matherials, Faculty of Electrical Engineering and Computing, University of Zagreb},
	year = 	 {2013}
}

@article{Pettersson2009,
	doi = {10.1016/j.ygeno.2008.10.003},
	url = {https://doi.org/10.1016/j.ygeno.2008.10.003},
	year  = {2009},
	month = {feb},
	publisher = {Elsevier {BV}},
	volume = {93},
	number = {2},
	pages = {105--111},
	author = {Erik Pettersson and Joakim Lundeberg and Afshin Ahmadian},
	title = {Generations of sequencing technologies},
	journal = {Genomics}
}

@article{Boza2017,
	doi = {10.1371/journal.pone.0178751},
	url = {https://doi.org/10.1371/journal.pone.0178751},
	year  = {2017},
	month = {jun},
	publisher = {Public Library of Science ({PLoS})},
	volume = {12},
	number = {6},
	pages = {e0178751},
	author = {Vladim{\'{\i}}r Bo{\v{z}}a and Bro{\v{n}}a Brejov{\'{a}} and Tom{\'{a}}{\v{s}} Vina{\v{r}}},
	editor = {Degui Zhi},
	title = {{DeepNano}: Deep recurrent neural networks for base calling in {MinION} nanopore reads},
	journal = {{PLOS} {ONE}}
}

@article {David046086,
	author = {David, Matei and Dursi, Lewis Jonathan and Yao, Delia and Boutros, Paul C and Simpson, Jared T},
	title = {Nanocall: An Open Source Basecaller for Oxford Nanopore Sequencing Data},
	year = {2016},
	doi = {10.1101/046086},
	publisher = {Cold Spring Harbor Labs Journals},
	abstract = {Motivation: The highly portable Oxford Nanopore MinION sequencer has enabled new applications of genome sequencing directly in the field. However, the MinION currently relies on a cloud computing platform, Metrichor (metrichor.com), for translating locally generated sequencing data into basecalls. Results: To allow offline and private analysis of MinION data, we created Nanocall. Nanocall is the first freely-available, open-source basecaller for Oxford Nanopore sequencing data and does not require an internet connection. On two ecoli and two human samples, with natural as well as PCR-amplified DNA, Nanocall reads have ~68\% identity, directly comparable to Metrichor "1D" data. Further, Nanocall is efficient, processing ~500Kbp of sequence per core hour, and fully parallelized. Using 8 cores, Nanocall could basecall a MinION sequencing run in real time. Metrichor provides the ability to integrate the "1D" sequencing of template and complement strands of a single DNA molecule, and create a "2D" read. Nanocall does not currently integrate this technology, and addition of this capability will be an important future development. In summary, Nanocall is the first open-source, freely available, off-line basecaller for Oxford Nanopore sequencing data. Availability: Nanocall is available at github.com/mateidavid/nanocall, released under the MIT license. Contact: matei.david at oicr.on.ca},
	URL = {http://biorxiv.org/content/early/2016/03/28/046086},
	eprint = {http://biorxiv.org/content/early/2016/03/28/046086.full.pdf},
	journal = {bioRxiv}
}

@manual{rnn-blog,
	author =       {Britz, Denny},
	title =        {Recurrent Neural Networks Tutorial - Backpropagation Through Time and Vanishing Gradients},
	editor =       {wildml.com},
	url={https://goo.gl/Qkv9gC},
	note = {[Online; posted 15-September-2015]}},
}


@inproceedings{Graves:2006:CTC:1143844.1143891,
	author = {Graves, Alex and Fern\'{a}ndez, Santiago and Gomez, Faustino and Schmidhuber, J\"{u}rgen},
	title = {Connectionist Temporal Classification: Labelling Unsegmented Sequence Data with Recurrent Neural Networks},
	booktitle = {Proceedings of the 23rd International Conference on Machine Learning},
	series = {ICML '06},
	year = {2006},
	isbn = {1-59593-383-2},
	location = {Pittsburgh, Pennsylvania, USA},
	pages = {369--376},
	numpages = {8},
	url = {http://doi.acm.org/10.1145/1143844.1143891},
	doi = {10.1145/1143844.1143891},
	acmid = {1143891},
	publisher = {ACM},
	address = {New York, NY, USA},
} 



@InProceedings{graves_decode,
	title = 	 {Towards End-To-End Speech Recognition with Recurrent Neural Networks},
	author = 	 {Alex Graves and Navdeep Jaitly},
	booktitle = 	 {Proceedings of the 31st International Conference on Machine Learning},
	pages = 	 {1764--1772},
	year = 	 {2014},
	editor = 	 {Eric P. Xing and Tony Jebara},
	volume = 	 {32},
	number =       {2},
	series = 	 {Proceedings of Machine Learning Research},
	address = 	 {Bejing, China},
	month = 	 {22--24 Jun},
	publisher = 	 {PMLR},
	pdf = 	 {http://proceedings.mlr.press/v32/graves14.pdf},
	url = 	 {http://proceedings.mlr.press/v32/graves14.html},
	abstract = 	 {This paper presents a speech recognition system that directly transcribes audio data with text, without requiring an intermediate phonetic representation. The system is based on a combination of the deep bidirectional LSTM recurrent neural network architecture and the Connectionist Temporal Classification objective function. A modification to the objective function is introduced that trains the network to minimise the expectation of an arbitrary transcription loss function. This allows a direct optimisation of the word error rate, even in the absence of a lexicon or language model. The system achieves a word error rate of 27.3% on the Wall Street Journal corpus with no prior linguistic information, 21.9% with only a lexicon of allowed words, and 8.2% with a trigram language model. Combining the network with a baseline system further reduces the error rate to 6.7%.}
}

@article{hochreiter1997long,
	added-at = {2016-11-15T08:49:43.000+0100},
	author = {Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
	biburl = {https://www.bibsonomy.org/bibtex/2a4a80026d24955b267cae636aa8abe4a/dallmann},
	interhash = {0692b471c4b9ae65d00affebc09fb467},
	intrahash = {a4a80026d24955b267cae636aa8abe4a},
	journal = {Neural computation},
	keywords = {lstm rnn},
	number = 8,
	pages = {1735--1780},
	publisher = {MIT Press},
	timestamp = {2016-11-15T08:49:43.000+0100},
	title = {Long short-term memory},
	volume = 9,
	year = 1997
}


@manual{ctc-blog,
	author =       {Gibiansky, Andrew},
	title =        {Speech Recognition with Neural Networks},
	editor =       {http://andrew.gibiansky.com},
	url={http://andrew.gibiansky.com/blog/machine-learning/speech-recognition-neural-networks/},
	note = {[Online; posted 23-April-2014]}},
}

@misc{resnet,
	Author = {Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},
	Title = {Deep Residual Learning for Image Recognition},
	Year = {2015},
	Eprint = {arXiv:1512.03385},
}


@misc{prelu,
	Author = {Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},
	Title = {Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification},
	Year = {2015},
	Eprint = {arXiv:1502.01852},
}

@misc{elu,
	Author = {Djork-Arné Clevert and Thomas Unterthiner and Sepp Hochreiter},
	Title = {Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)},
	Year = {2015},
	Eprint = {arXiv:1511.07289},
}

@misc{adam,
	Author = {Diederik P. Kingma and Jimmy Ba},
	Title = {Adam: A Method for Stochastic Optimization},
	Year = {2014},
	Eprint = {arXiv:1412.6980},
}

@misc{batch norm
	Author = {Sergey Ioffe and Christian Szegedy},
	Title = {Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift},
	Year = {2015},
	Eprint = {arXiv:1502.03167},
}
@misc{identitet,
	Author = {Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},
	Title = {Identity Mappings in Deep Residual Networks},
	Year = {2016},
	Eprint = {arXiv:1603.05027},
}
@article{resnet-elu,
	Author = {Anish Shah and Eashan Kadam and Hena Shah and Sameer Shinde and Sandip Shingade},
	Title = {Deep Residual Networks with Exponential Linear Unit},
	Year = {2016},
	Eprint = {arXiv:1604.04112},
	Doi = {10.1145/2983402.2983406},
}

@misc{facebook,
	Author = {Jonas Gehring and Michael Auli and David Grangier and Denis Yarats and Yann N. Dauphin},
	Title = {Convolutional Sequence to Sequence Learning},
	Year = {2017},
	Eprint = {arXiv:1705.03122},
}

@misc{BNORM,
	Author = {Sergey Ioffe and Christian Szegedy},
	Title = {Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift},
	Year = {2015},
	Eprint = {arXiv:1502.03167},
}

@misc{end_to_end_conv_ctc,
	Author = {Ying Zhang and Mohammad Pezeshki and Philemon Brakel and Saizheng Zhang and Cesar Laurent Yoshua Bengio and Aaron Courville},
	Title = {Towards End-to-End Speech Recognition with Deep Convolutional Neural Networks},
	Year = {2017},
	Eprint = {arXiv:1701.02720},
}
@misc{selu,
	Author = {Günter Klambauer and Thomas Unterthiner and Andreas Mayr and Sepp Hochreiter},
	Title = {Self-Normalizing Neural Networks},
	Year = {2017},
	Eprint = {arXiv:1706.02515},
}
@misc{bytenet,
	Author = {Nal Kalchbrenner and Lasse Espeholt and Karen Simonyan and Aaron van den Oord and Alex Graves and Koray Kavukcuoglu},
	Title = {Neural Machine Translation in Linear Time},
	Year = {2016},
	Eprint 