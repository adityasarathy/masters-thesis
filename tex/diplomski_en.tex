\documentclass[times, utf8, diplomski, numeric, english]{fer}

\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{nccmath}
%\usepackage{hyperref}

\usepackage{footnote}
\usepackage{graphicx}




\DeclareMathOperator*{\argmin}{\arg\!\min}
\DeclareMathOperator*{\argmax}{\arg\!\max}
\begin{document}


\thesisnumber{1417}
\title{Deep Learning Model for Base Calling of MinION Nanopore Reads}
\author{Marko Ratković}

\maketitle

% Ispis stranice s napomenom o umetanju izvornika rada. Uklonite naredbu \izvornik ako želite izbaciti tu stranicu.
\izvornik

% Dodavanje zahvale ili prazne stranice. Ako ne želite dodati zahvalu, naredbu ostavite radi prazne stranice.
\zahvala{Thanks ...}

\tableofcontents
\listoffigures
\listoftables

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% CHAPTER
\chapter{Introduction}

In recent years,  deep learning and usage of deep neural networks have significantly improved the state-of-the-art in many application domains such as computer vision, speech recognition, and natural language processing\cite{LeCun:1998:CNI:303568.303704}\cite{NIPS2012_4824}. 
In this thesis, we present application of deep learning in the fields of Biology and Bioinformatics for analysis of DNA sequencing data. 

DNA is a molecule that makes up the genetic material of a cell responsible for carrying the information an organism needs to survive, grow and reproduce. 
It is a long polymer of simple units called nucleotides attached together to form two long strands that spiral to create a structure called a double helix. The order of these bases is what determines DNA's instructions, or genetic code.

DNA sequencing is the process of determining this sequence of nucleotides. Originally sequencing was very expensive process but 
during the last couple of decades, the price of sequencing drastically dropped. A significant breakthrough occurred in May 2015 with the release of MinION sequencer by Oxford Nanopore making DNA sequencing inexpensive and available even for small research teams. 

Base calling is a process assigning sequence of nucleotides (bases) to the raw data generated by the sequencing device or sequencer. Simply put, it is a process of decoding the raw output from the sequencer.


\section{Objectives}
Goal of this thesis is to show that the accuracy of base calling is much dependent on the underlying software and can be improved using machine learning methods. Novel approach for base calling of raw data using convolutional neural networks is presented.
 
\section{Organization}
\indent Chapter 2 gives more detailed explanation of the problem, background on nanopore sequencing and overview of state-of-the-art basecallers.

Chapter 3 describe used deep learning concepts in detail used later on in later chapters.

Chapter 4 goes into implementation details, training of the deep learning model and explains methods used to evaluate obtained results. 

Chapter 5 consists of the results of testing performed on different datasets as well as comparison with state-of-the-art basecallers.

In the end, the Chapter 6 gives a brief conclusion and possible future work and improvements of the developed basecaller.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% CHAPTER
\chapter{Background}

\section{Sequencing}

Sequencing the entire genome of an organism is a difficult problem due to limitations of technology.
All sequencing technologies to date have constraints on the length of the strand they can process. 
These lengths are much smaller than the genome for a majority of organisms, therefore, whole genome shotgun sequencing approach is used. 
In this approach, multiple copies of the genome are broken up randomly into numerous small fragments that can be processed by the sequencer. Sequenced fragments are called reads.

Genome assembly is the process of reconstructing the original genome from reads and usually starts with finding overlaps between reads.
The quality of reconstruction heavily depends on the length and accuracy of the reads produced by the sequencer. Short reads make resolving repetitive regions practically impossible.

Figure \ref{fg:sequencing} depicts process of sequencing.

\begin{figure}[!ht]
	\begin{center}
		\includegraphics[width=0.6\textwidth]{./imgs/sequencing.png}
		\caption{Depiction of the sequencing process}
		\label{fg:sequencing}
	\end{center}
\end{figure}


Development of sequencing started with work of Frederick Sanger\cite{mile}\cite{Pettersson2009}. In 1977, he developed the first sequencing method which allowed 
read lengths up to 1000 bases with very high accuracy (99.9\%) at a cost of 1\$ per 1000 bases[mile\_skripta].
Second generation sequencing (IAN Torrent and Illumina devices) reduced the price of sequencing while maintaining high accuracy. Mayor disadvantage of these devices is read length of only a few hundred base pairs.
The need for technologies that would produce longer reads led to the development of so-called third generation sequencing technologies.
PacBio developed sequencing method that allowed read lengths up to several thousand bases but at a cost of accuracy. Error Rates of PacBio devices are ~10-15\%. 

Cost makes the main obstacle stopping widespread genome sequencing.
A significant breakthrough occurred in May 2015 with a release of MinION sequencer by Oxford Nanopore making sequencing drastically less expensive and even portable.


\section{Oxford Nanopore MinION}


The MinION device by Oxford Nanopore Technologies (referenca) is the first portable DNA sequencing device.

It’s small weight, low cost, and long read length combined with high-throughput and decent accuracy yield promising results in various applications such as monitoring infectious disease outbreaks [2][3], characterizing structural variants in cancer[4], full human genome assembly [5] what could potentially lead to personalized genomic medicine.

Although MinION is able to produce long reads, usually tens of thousand
base pairs (with reported reads above 100 thousand pairs[loman]), they have a high sequencing error
rate depending of the pore model used. Older pore models, R7.3 chemistry.....%TODO



As its name says, nano-scale pores are used to sequence DNA. An electrical potential is applied over an insulating membrane in which a  pore is inserted. As the DNA passes through the pore, the sensor detects changes in ionic current caused by differences in the shifting nucleotide sequences occupying the pore. Figure bla shows change of ionic current as DNA strain being pulled through a nanopore.

\subsection{Technology}
As its name says, nano-scale pores are used to sequence DNA. An electrical potential is applied over an insulating membrane in which a  pore is inserted. As the DNA passes through the pore, the sensor detects changes in ionic current caused by differences in the shifting nucleotide sequences occupying the pore. Figure bla shows change of ionic current as DNA strain being pulled through a nanopore.

\begin{figure}[!ht]
	\begin{center}
		\includegraphics[width=0.8\textwidth]{./imgs/nanopore.png}
		\caption{DNA strain being pulled through a nanopore[ref na video])}
		\label{fg:nanopore}
	\end{center}
\end{figure}



\section{Existing basecallers}

Output of the sequencer is fast5 file. Various fields, including signal.
Picture bla shows structure of fast5 file. Analysis fields are added if file is passed to MinKnow for signal segmentation.

\begin{figure}[!ht]
	\begin{center}
		\includegraphics[width=0.5\textwidth]{./imgs/basecalling.png}
		\caption{Basecalling}
		\label{fg:basecalling}
	\end{center}
\end{figure}

\subsection{Official}
\textbf{Albacore} is basecaller by Oxford Nanopore Technologies ready for production and activly supported.
The source code of Albacore was not provided and is only available as a binary through the ONT Developer Channel to users who have signed the Developer terms and conditions. 

\textbf{Nanonet} uses the same neural network that is used in Albacore but it is continually under development. As such, it does not contain production code features such as error handling or logging. It uses CURRENNT library for running neural networks.


\textbf{Scrappie} is an other basecaller by Oxford Nanopore Technologies. Simillar to NanoNet, it is platform for ongoing development. Scrappie was the first basecaller reported to specifically address homopolymer basecalling. It became publicly available just recently in June, 2017.

\subsection{Third-party}

\textbf{Nanocall}\cite{David046086} was the first third-party open source basecaller for nanopore data. It uses HMM approch like the original R7 Metrichor. Metrichor. It uses the segmented signal from minKNOW and assigns k-mers to the events using a hidden Markov model. 
Nanocall does not support newer chemistries after R7.3 directly, it can be used but its accuracy is significantly lower than other basecallers.
 

\textbf{DeepNano}\cite{Boza2017}  was the first open-source basecaller based on neural networks that uses bidirectional recurent neural networks. DeepNano was written in Python, using the Theano library. When released, originaly supported R7 chemistry, but support for R9.4 and R9.5 was added recently.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% CHAPTER
\chapter{Methods}
Process of basecalling can be represented as problem of machine translation where sentence is translated from one language (sequence of events or sequence of current measurements) to an other (sequence of nucleotides). 

Instead of opting for the traditional path using RNN (recurent neural networks)  we tried using CNN(Convolutional neural networks). 


\section{Arhitecture}
This section gives general idea behind recurrent neural networks and possible problems that serve as motivation for the different approach - usage of convolutional neural networks.

\subsection{RNN}
Recurrent neural networks can be viewed as a simple feed-forward network with the twist that the current output does not only depend on the current input but previous inputs as well. RNNs store that information in their hidden state and that state is updated each step. The figure shows simple RNN and the same RNN unfolded in time.  Unrolling is simple way of showing how network processes each input in the sequence and updates it's hidden state and is shown in figure  \ref{fg:rnn}. 
\begin{figure}[!ht]
	\begin{center}
		\includegraphics[width=0.8\textwidth]{./imgs/rnn.png}
		\caption{An unrolled recurrent neural network}
		\label{fg:rnn}
	\end{center}
\end{figure}

These networks are trained using a variant of backpropagation called backpropagation through time which is essentially the same as classical backpropagation on an unfolded network. The gradient is propagated through the entire recurrence relation and the gradient is multiplied in each step with the same factor, depending on a scale it can make gradient vanish (drop to 0) or exponentially grow each step and explode. These issues are called the vanishing and exploding gradient\cite{rnn-blog} and are generally resolved by a variant of RNN called \textit{LSTM}\cite{hochreiter1997long}. To take into account dependencies of both previous inputs and next inputs in the sequence bidirectional networks (\textit{BRNN}) are used.  Idea is to combine two RNN (one in the positive direction, one in negative time direction) and have an output of the current state expressed as a function of hidden states of both RNN and current input. This is the approach used in DeepNano\cite{Boza2017}. 


One of the major drawbacks of this kind of networks is computation time. RNN operate sequentially, the output for the second step depends on the first step and so on, which makes parallelization capabilities of RNN quite limited. This especially goes for Bidirectional RNN.

\subsection{CNN}
Convolutional Neural Network (CNNs) typically used for various Computer Vision. CNNs were responsible for major breakthroughs in Image Classification and are the core of most Computer Vision systems today. More recently we’ve also started to apply CNNs to problems in Natural Language Processing and gotten some interesting results.

Easiest way to understand a convolution is by thinking of it as a sliding window function applied to a matrix or in case of signal processing vector. The sliding window is called a kernel or a filter. Figure \ref{fg:convolution} shows example of convolution with kernel size 3 and how output is calculated as sum of element-wise multiplication of kernel elements and input vector.
\begin{figure}[!ht]
	\begin{center}
		\includegraphics[width=0.5\textwidth]{./imgs/convolution.png}
		\caption{Convolution layer, kernel size 3}
		\label{fg:convolution}
	\end{center}
\end{figure}

When compered with RNN, in which output can depend on the entire sequence in convolution layer if depends only on limited spatial information in previous layer defined by kernel size. This is called receptive field of the convolution. 
To see more of the original input, convolutions are stacked stacked. Figure \ref{fg:receptive field} shows each new layers depends on larger portion of the input($z_i$ \textit{sees} 5 elements of input).
\begin{figure}[!ht]
	\begin{center}
		\includegraphics[width=0.5\textwidth]{./imgs/receptive_field.png}
		\caption{Receptive field after 2 layers of convolutions with kernel size 3}
		\label{fg:receptive field}
	\end{center}
\end{figure}

During calculation, each “patch” a convolutional kernel operates on is independent of the other meaning that the entire input layer can be processed concurrently. 

CNNs are basically just several layers of convolutions with nonlinear activation functions.


As previously show, stacking layers increases receptive field. 
Neural networks are networks that information flows through. In the forward pass our input flows and transforms, hopefully becoming a representation that is more amenable to our task. During the back phase we propagate a signal, the gradient, back through the network. Just like in RNNs, that signal gets multiplied and depending on the scales it can vanish. Result of that is no gradient flowing to lower layers and no parameter upgrades. On the one hand, we’d like to be able to take in as much context as possible. On the other hand, if we try to increase our receptive fields by stacking layers we risk vanishing gradients and a failure to learn anything. Resnet arhitecture\cite{resnet} and its residual layes address this issue.

\subsection{Residual Networks}
A Residual Network, or ResNet is a neural network architecture which solves the problem of vanishing gradients in the simplest way possible. 

Figure bla on the left shows classical CNN that takes input and transformes it using convolution layers and activation. 
This can be presentent as H(x) function from input to output. 
H(x) can be written as F(X) + X.  F(X) is called The residual.
\begin{figure}[!ht]
	\begin{center}
		\includegraphics[width=0.7\textwidth]{./imgs/resnet_block.png}
		\caption[Comparison between classical CNN and CNN with residual connection ]{Comparison between classical CNN and CNN with residual connection \protect\footnotemark}
		\label{fg:receptive field}
	\end{center}
\end{figure}
\footnotetext{Figure adapted from the original paper \cite{resnet}}
Instead of Networks learns residual and at the output x is simply arithmetically added to the F(x). 
During backpropagation, learning residual gives us nice property. By stacking these layers, the gradient could theoretically “skip” over all the intermediate layers and reach the bottom without being diminished.


\subsection{Gated Residual Networks}
Why not use only needed - distribution of gates

\section{CTC Loss}

The goal is to design model which can convert from a sequence of events of current measurements into a sequence of base pairs.

Suppose that we have an input sequence $X$ (signal data) and the desired output sequence $Y$ (nucleotides). $X$ and $Y$ will be of different lengths(the length of base pairs is always smaller than the length of the raw signal), which may pose a problem.

Instead of having a variable size of the output from the neural network, we can limit it to length $m$ (maximal length of output sequence $Y$).
The neural network can be considered to be simply a function that takes in some input sequence $X$ (of length $n$) and generate some sequence $O$ (of length $m$). Note that this generated sequence is not same as output sequence $Y$. 

\subsection{Definition}
They key idea behind Connectionist Temporal Classification(CTC)\cite{Graves:2006:CTC:1143844.1143891} is that instead of directly generating output sequence $Y$ as output from the neural network, we  generate a probability distribution at every output length (from $t$=1 to $t$=$m$) that after \textit{decoding} gives maximum likelihood output sequence $Y$. 
Finally, network is trained by creating an objective function that restricts the maximum likelihood decoding for a given sequence $X$ to correspond to our desired target sequence $Y$.


Given an input sequence $X$ of length $n$, the network generates some probabilities over all possible labels (A, C, T, and G) with an extra symbol representing a "blank" at each timestep. 

The output generated by the network is called \textit{path}. Path is defined by the sequence of it's elements $\pi = (\pi_1, \pi_2, ..., \pi_m)$.
The probability of a given path $\pi$, given inputs $X$, can then be written as the product of all its forming elements:

\begin{gather*}
P(\pi | X) = \prod_{t=1}^{m} o_t(\pi_t), \\
\text{where $o_t(\pi_t)$ is probability of $\pi_t$ being $t^{th}$ element on path $\pi$} 
\end{gather*}

Real output sequence, for given path, is obtained by traversing the path and removing all blanks and duplicate letters. Let $ decode(\pi) $ be the output sequence corresponding to a path $\pi$. The probability of output sequence $Y$ is then the sum of probabilities of all paths that decode to $Y$:
\begin{gather*}
P(Y | X) = \sum_{\pi \in decode^{-1}(Y)}^{} P(\pi | X)
\end{gather*}

\subsection{Objective}
Given the dataset $D = \{(X, Y)\}$, training objective is the maximization of the likelihood of each training sample which corresponds to the minimization of negative log likelihood:

\begin{gather*}
L(D) = - \sum_{(X,Y)\in D}^{} ln P(Y | X)
\end{gather*}


\subsection{Output decoding}
Given the probability distribution $P(Y | X)$ and given input sequence $X$, most likely $Y^{*}$ can be computed.
\begin{gather*}
Y^{*} = \argmax_{Y \in L^m} P(Y|X) = \argmax_{Y \in L^m} \sum_{\pi \in decode^{-1}(Y)}^{} P(\pi | X),\\
\text{~where $L^m$ set of all possible sequences over alphabet $L$ with length less than or equals to $m$}
\end{gather*}

The probability of a single output sequence $Y$ is the sum of probabilities of all paths that decode to $Y$ and the most probable sequence is needed to be found.
Calculation of all possible sequences is computationally intractable but exist algorithms that approximate decoding. 

One naive possibility is to take the most probable path and say that output sequence corresponds to that path.
This is not necessarily true: suppose we have one path with probability $0.1$ corresponding with sequence $A$, and ten paths with probabilities  $0.05$ each corresponding to sequence $B$. Clearly, label $B$ is preferable overall, since it has an overall probability of $0.5$; however, this naive best path decoding would select label $A$, which has a higher probability than any single path for label $B$.

Better approximations can be calculated using beam search algorithm proposed in paper\cite{Graves:2006:CTC:1143844.1143891}.  
Idea behind this approach is tu 

This serves as a brief overview of the method and explains key concepts why it is used. More detailed explanation can be found in original paper\cite{Graves:2006:CTC:1143844.1143891} or various blog post\cite{ctc-blog}.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% CHAPTER
\chapter{Implementation}
\section{Deep Learning model}
\section{Training}

To train described model we need to obtain dataset that consist of sample pairs $(X_i, Y_i)$ where $(X_i)$ is input signal and $Y_i$ is output sequence. 
One of the mayor issues is determining correct output for given signal. One option is to use existing basecaller like Metrichor to determine output sequences. 

 




Supervised learning so for need to specify dataset that for each input signal we need specify desired sequence of bases.
{Xi, Yi} where len of X and len of Yi are not specified.

For each input file, ground truth is not specified. If we use output of Metrihor or any other basecaller we limit our model to
obtain accuracy of used basecaller in best case. We limit our train data to only sequencing data of know organisms (organisms with know reference genome) and try to correct data by aligning the read produced by metrichor or any other basecaller to reference genome. Alignment destination is used as target sequencing in training. 
Using Metrichor basecalled data we can determine for each called event values such as start in signal, length of event, k-mer state in the pore and using move field change of k-mer from previous state. 

Used dataset for training consists of raw signal from fast5 files split raw data into blocks of size $l$.
For each block it is easy to determine basecalled events from Metrichor that belong to particular block using start information and from those events basecalled sequence. 


$block\_index =  \frac{event_{start} * sampling\_rate}{block\_size}$

Full basecalled sequence is aligned to the reference genome and alignment was obtained. For each block target sequence is determined from alignment information.

Figure X shows how for given signal block


Figure bla shows augmentation of data.

To eliminate possibility of overfitting to the know reference, model is trained and tested on reads that align to different regions of reference genome and those regions should not overlap. Also test is conducted on separate set of sequencing data for different organism than the one used for training. 

\begin{figure}[!ht]
	\begin{center}
		\includegraphics[width=1\textwidth]{./imgs/train_data_correction.png}
		\caption{Dataset preparation}
		\label{fg:data_correction}
	\end{center}
\end{figure}
\begin{figure}[!ht]
	\begin{center}
		\includegraphics[width=0.7\textwidth]{./imgs/train_pipeline.png}
		\caption{Overview of training pipeline}
		\label{fg:train_pipe}
	\end{center}
\end{figure}
\section{Evaluation methods}
To evaluate trained model we base call test set, align those reads to reference and calculate various statistics on align data.
From cigar string it is easy to calculate following:

\begin{gather*}
read\_len =  n\_matches + n\_missmatches + n\_insertions \\
match\_rate = \dfrac{n\_matches}{read\_lenght} \\
missmatch\_rate = \dfrac{n\_missmatches}{read\_lenght}\\
insertion\_rate = \dfrac{n\_insertions}{read\_lenght}\\
deletion\_rate = \dfrac{n\_deletion}{read\_lenght}\\
match\_rate = \dfrac{n\_matches}{read\_lenght}\\
\end{gather*}

Results are calculated for each read in test dataset and mean value and standard deviation is expressed for the whole dataset.

To validate consistency of a basecaller, basecalled data is aligned to the reference genome and consensus sequence is called from all reads covering single position. Consensus sequence is compared with the reference genome and following measures are calculated:
\begin{gather*}
identity\_percentage =  100 * \dfrac{n\_correct\_bases}{reference\_lenght} \\
match\_rate = \dfrac{n\_correct\_bases}{consensus\_lenght} \\
snp\_rate = \dfrac{n\_snp}{consensus\_lenght}\\
insertion\_rate = \dfrac{n\_insertions}{consensus\_lenght}\\
deletion\_rate = \dfrac{n\_deletion}{consensus\_lenght}\\
\end{gather*}

\begin{figure}[!ht]
	\begin{center}
		\includegraphics[width=0.8\textwidth]{./imgs/consnesus.png}
		\caption{Consensus from pileup}
		\label{fg:consensus}
	\end{center}
\end{figure}


\begin{figure}[!ht]
	\begin{center}
		\includegraphics[width=0.7\textwidth]{./imgs/evaluation_pipeline.png}
		\caption{Overview of evaluation pipeline}
		\label{fg:eval_pipe}
	\end{center}
\end{figure}

\section{Technologies}

Overall solution was implemented in Python programing language. Described model is implemented using TensorFlow. Tensorflow is an open source software library for numerical computation using data flow graphs developed by Google.  TensorFlow, even tho is considered low-level framework offers implementations of higher level concepts (layers, losses, and optimizers) which makes it great for prototyping while keeping it modular and extensible for highly specific tasks as well.

Tensorflow offers efficient GPU implementations of various layers and losses but as of version 1.2 lacks GPU implementation of used CTC loss, so WARP-CTC (https://github.com/baidu-research/warp-ctc) was used. It offers both GPU and CPU implementations as well as bindings for Tensorflow.

For alignment tasks, developed tool offers support for GraphMap and BWA but can easily be extended with any other aligner that outputs results in Sam file format.

SAMTools(link) and its python bindings PySam(link) were used for conversions between various file formats used in Bioinformatics.

Docker was used for automating the deployments on different machines. It helps us resolve problem know as  "dependency hell"(link)  keeping all dependencies in single container thus eliminating possible conflict between packages on host OS.
Nvidia docker was used for GPU support.

All training was done on the server with  Intel(R) Xeon(R) E5-2640 CPU, 600 GB of RAM and NVIDIA TITAN X Black with 6GB of GDDR5 memory and 2880 CUDA cores.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% CHAPTER
\chapter{Results}
\section{Data}
\section{Error rates per read}
\section{Consensus analysis}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% CHAPTER
\chapter{Conclusion}
Conclusion.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% DONE
\bibliography{references}
\bibliographystyle{unsrtnat}
\begin{abstract}
Abstract.

\keywords{Keywords.}
\end{abstract}

\hrtitle{Model dubokog učenja za određivanje očitanih baza dobivenih uređajem za sekvenciranje MinION}
\begin{sazetak}
Sažetak na hrvatskom jeziku.

\kljucnerijeci{Ključne riječi, odvojene zarezima.}
\end{sazetak}

\end{document}
